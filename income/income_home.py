# -*- coding:utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
from streamlit_option_menu import option_menu
from PIL import Image

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score


# ëª¨ë¸ë§ import
from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold, cross_val_score
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from collections import Counter
import plotly.express as px
from imblearn.under_sampling import TomekLinks, RandomUnderSampler
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.pipeline import Pipeline, make_pipeline
from imblearn.combine import SMOTETomek
from xgboost import XGBClassifier



from statsmodels.tools.tools import add_constant
from statsmodels.stats.outliers_influence import variance_inflation_factor


def income_run(total_df): 
    st.header('Income Decile Prediction')
    st.markdown("""---""")

    # Tab creation for different steps
    tab1, tab2, tab3, tab4 = st.tabs(["íŠ¹ì„±ê³µí•™(Feature Engineering)", "ë°ì´í„° ë¶„í¬ë„ í™•ì¸(Data Distribution)", "ìš”ì¸ ë¶„ì„(Factor Analysis)", "ëª¨ë¸ë§(Modeling)"])

    # Feature Engineering
    with tab1:
        st.markdown('### íŠ¹ì„±ê³µí•™(Feature Engineering)')


        # Split into two columns: VIF Data and VIF Plot
        top_col1, top_col2 = st.columns([3, 5], gap="small", vertical_alignment="center")

        col1, col2 = st.columns([3, 5], gap="small", vertical_alignment="center")
        

        with top_col1:
            st.markdown("#### VIF ë¶„ì„ ê²°ê³¼ ìˆœì„œ")

        with top_col2:
            st.markdown("#### VIF ë¶„ì„ ê²°ê³¼ ì‹œê°í™”")

        with col1:
            # Prepare data for VIF analysis
            X = total_df.drop('Income_code', axis=1)
            X = add_constant(X)

            # Calculate VIF
            vif_data = pd.DataFrame()
            vif_data["Feature"] = X.columns
            vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
            vif_data = vif_data.iloc[1:]  # ìƒìˆ˜í•­(constant)ì„ ì œê±°

            # VIF ë°ì´í„°í”„ë ˆì„ ì¶œë ¥
            st.write(vif_data)

        with col2:
            # VIF ì‹œê°í™”
            plt.figure(figsize=(4, 4))
            sns.barplot(x='VIF', y='Feature', data=vif_data)
            plt.title('Variance Inflation Factor (VIF)')
            plt.xticks(rotation=90)
            st.pyplot(plt)

    # PCA ë° t-SNE ë¶„í¬ ë¶„ì„
    with tab2:
        st.markdown('### ë°ì´í„° ë¶„í¬ë„ í™•ì¸')

        st.markdown("""
        - ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ì†Œë“ë¶„ìœ„(1~10ë¶„ìœ„)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ì˜ ë¶„í¬ë„ë¥¼ í™•ì¸.
        - PCAëŠ” ë°ì´í„°ì˜ ì „ë°˜ì ì¸ ë¶„í¬ë¥¼ ì´í•´í•˜ê³ , ì£¼ì„±ë¶„ì´ ë°ì´í„°ì˜ ë³€ë™ì„±ì„ ì–´ë–»ê²Œ ì„¤ëª…í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë° ìœ ìš©í•˜ê³ 
        - t-SNEëŠ” ë°ì´í„°ì˜ í´ëŸ¬ìŠ¤í„°ë§ì´ë‚˜ êµ­ì†Œì ì¸ ë°ì´í„° êµ¬ì¡°ë¥¼ ëª…í™•í•˜ê²Œ ë“œëŸ¬ë‚´ëŠ” ë° ì í•©í•˜ê¸° ë•Œë¬¸ì— ë‘ ê°€ì§€ì˜ ë¶„í¬ë„ë¥¼ ëª¨ë‘ í™•ì¸
            
        -> ì†Œë“ ë¶„ìœ„ ê²°ì • ìš”ì¸ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ ë§ì€ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹, ëª¨ë¸ë§ ìµœì í™” ë“±ì„ í•´ ë³¸ ê²°ê³¼ ì •í™•ë„ê°€ ì¢‹ì§€ ì•Šì€ ì›ì¸ì„ ì°¾ê¸° ìœ„í•´ ì‹œë„.
  
        ğŸ“Œì†Œë“ë¶„ìœ„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ PCA 2D(2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œëœ ë°ì´í„°) ë°ì´í„°ì˜ ë¶„í¬ë„ í™•ì¸ ê²°ê³¼

        - ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ì†Œë“ë¶„ìœ„(1 ~ 10ë¶„ìœ„)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ì—¬ PCA 2D ë°ì´í„°ì˜ ë¶„í¬ë„ë¥¼ í™•ì¸í–ˆì„ ë•Œ, ì•„ë˜ì˜ ê·¸ë˜í”„ì™€ ê°™ì´ ê° 1 ~ 10ë¶„ìœ„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ì˜ ë¶„í¬ê°€ ì •í™•í•˜ê²Œ êµ¬ë¶„ë˜ì§€ ì•Šì•˜ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.
        - ì˜ˆë¥¼ ë“¤ì–´, ì†Œë“ë¶„ìœ„ 1ë¶„ìœ„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ëŠ” 2ë¶„ìœ„ì˜ ë²”ìœ„ì—ë„ í¬í•¨ë˜ì–´ ìˆëŠ” ê²½ìš°ê°€ ë°œê²¬ë˜ë©° ë‹¤ë¥¸ 2, 3, 4, 5, 6, 7, 8, 9, 10 ë¶„ìœ„ ë²”ìœ„ì— í¬í•¨ëœ ë°ì´í„°ë“¤ë„ ê²½ê³„ê°€ ëª¨í˜¸í•˜ê²Œ ì„ì—¬ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.
        
        â­ ê²°ë¡ 
        - ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” ë°ì´í„°ëŠ” ê·¸ ê°’ë“¤ì˜ ê²½ê³„ê°€ ëª¨í˜¸í•˜ê¸° ë•Œë¬¸ì— ì†Œë“ë¶„ìœ„ë¥¼ ì˜ˆì¸¡í•˜ê³ ì í•  ë•Œ, ì •í™•í•˜ê²Œ ê° ë¶„ìœ„ë³„ë¡œ êµ¬ë¶„ì„ í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ ì‹¤ì œ ì†Œë“ë¶„ìœ„ì™€ ìœ ì‚¬í•˜ê²Œ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤ëŠ” í•œê³„ì ì´ ë°œìƒí–ˆë‹¤.
        """)

        # í•œ í˜ì´ì§€ì— ë‘ê°œì˜ ìš”ì†Œë¥¼ ì¶œë ¥í•˜ë„ë¡ ë¶„í• 
        col1, col2 = st.columns(2, gap="large")

        with col1:

            # ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ë° PCA ìˆ˜í–‰
            scaler = StandardScaler()
            scaled_data = scaler.fit_transform(total_df.drop('Income_code', axis=1))

            # PCA ë¶„í¬ ì‹œê°í™”
            st.subheader("PCA 2D Data Distribution")
            pca = PCA(n_components=2)
            pca_data = pca.fit_transform(scaled_data)

            plt.figure(figsize=(10, 8))
            sns.scatterplot(x=pca_data[:, 0], y=pca_data[:, 1], hue=total_df['Income_code'], palette=sns.color_palette("Spectral", 10))
            plt.title('PCA 2D Data Distribution')
            plt.xlabel('PCA Component 1')
            plt.ylabel('PCA Component 2')
            st.pyplot(plt)


        with col2:
            # t-SNE ë¶„í¬ ì‹œê°í™”
            st.subheader("t-SNE 2D Data Distribution")
            img = Image.open('data/t-SNE 2D.png')
            st.image(img, caption='t-SNE 2D ê²°ê³¼')


    # ìš”ì¸ ë¶„ì„
    with tab3:

        st.markdown("### ìš”ì¸ ë¶„ì„ (Random Forest Regression)")
        X = total_df.drop('Income_code', axis=1)
        y = total_df['Income_code']

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
        model = RandomForestRegressor(random_state=42)

        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [None, 10, 20, 30],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }

        # GridSearchCVëŠ” ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë ¤ì„œ ìƒëµ
        '''
        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')
        grid_search.fit(X_train, y_train)
        

        best_model = grid_search.best_estimator_
        y_pred = best_model.predict(X_test)
        st.write(grid_search.best_estimator_, grid_search.best_score_, grid_search.best_params_)

        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        feature_importances = pd.DataFrame(best_model.feature_importances_, index=X.columns, columns=['importance']).sort_values('importance', ascending=False)
        '''
        col1, col2 = st.columns(2, gap="large")

        with col1:

            st.markdown("### ìµœì ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ê°’")
            grid_best_param = {
                "max_depth":10,
                "min_samples_leaf":4,
                "min_samples_split":2,
                "n_estimators":300
            }
            st.write(pd.DataFrame.from_dict(grid_best_param, orient='index', columns=['Value']))

        
        with col2:
            evaluation_metrics = {
                'Mean Squared Error (MSE)': 1.4362,
                'Mean Absolute Error (MAE)': 0.869,
                'R^2 Score': 0.7371
            }
            
            st.markdown("### ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ")
            st.write(pd.DataFrame.from_dict(evaluation_metrics, orient='index', columns=['Score']))

    
    
        # í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”
        st.subheader("Feature Importances in Random Forest Regression")
        img = Image.open('data/randomforest_regression.png')
        st.image(img, caption='randomforest_regressionê²°ê³¼', width=900)
        

    # ëª¨ë¸ë§
    with tab4:
        st.markdown("### ì†Œë“ë¶„ìœ„ ì˜ˆì¸¡ ëª¨ë¸ë§")
        
        def evaluate_model(model, X_test, y_test, model_name):
            y_pred = model.predict(X_test)
            y_pred_prob = model.predict_proba(X_test)[:, 1]

            accuracy = metrics.accuracy_score(y_test, y_pred)
            precision = metrics.precision_score(y_test, y_pred, average='weighted')
            recall = metrics.recall_score(y_test, y_pred, average='weighted')
            f1 = metrics.f1_score(y_test, y_pred, average='weighted')
            roc_auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr')

            st.write(f'{model_name} í‰ê°€ ì§€í‘œ:')
            st.write(f'Accuracy: {accuracy:.4f}')
            st.write(f'Precision (Weighted): {precision:.4f}')
            st.write(f'Recall (Weighted): {recall:.4f}')
            st.write(f'F1 Score (Weighted): {f1:.4f}')
            st.write(f'ROC-AUC: {roc_auc:.4f}')

            # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
            st.subheader(f'{model_name} Confusion Matrix')
            cm = metrics.confusion_matrix(y_test, y_pred)
            plt.figure(figsize=(10, 8))
            sns.heatmap(cm, annot=True, fmt="d", linewidths=.5, cmap='Greens', square=True, cbar=False)
            plt.xlabel('Predicted Label')
            plt.ylabel('True Label')
            plt.title(f'{model_name} Confusion Matrixs')
            st.pyplot(plt)

            # ROC Curve ì‹œê°í™”
            st.subheader(f'{model_name} ROC Curve')
            fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_prob, pos_label=model.classes_[1])
            plt.figure(figsize=(8, 6))
            plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (area = {roc_auc:.4f})')
            plt.plot([0, 1], [0, 1], color='red', linestyle='--')
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'{model_name} ROC Curve')
            plt.legend(loc="lower right")
            st.pyplot(plt)

        X = total_df.drop('Income_code', axis=1)
        y = total_df['Income_code'] - 1

        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        # Logistic Regression ëª¨ë¸
        st.markdown("#### Logistic Regression")
        lr = LogisticRegression()
        lr.fit(X_train, y_train)
        evaluate_model(lr, X_test, y_test, 'Logistic Regression')

        # XGBoost ëª¨ë¸
        st.markdown("#### XGBoost")
        xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05, max_depth=3, eval_metric='mlogloss')
        xgb.fit(X_train, y_train)
        evaluate_model(xgb, X_test, y_test, 'XGBoost')

        # Random Forest ëª¨ë¸
        st.markdown("#### Random Forest")
        forest = RandomForestClassifier(n_estimators=200, random_state=42)
        forest.fit(X_train, y_train)
        evaluate_model(forest, X_test, y_test, 'Random Forest')

        # Sampling ì ìš©í•œ Random Forest ëª¨ë¸
        st.markdown("#### Sampling data Random Forest")
        smote = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))
        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
        forest.fit(X_resampled, y_resampled)
        evaluate_model(forest, X_test, y_test, 'Random Forest with Sampling')

        # Balanced Random Forest
        st.markdown("#### Balanced Random Forest")
        balanced_forest = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')
        balanced_forest.fit(X_train, y_train)
        evaluate_model(balanced_forest, X_test, y_test, 'Balanced Random Forest')

        '''
        # LSTM ëª¨ë¸ ê²°ê³¼ ì‹œê°í™” (ì™¸ë¶€ ì´ë¯¸ì§€ë¡œ ê°€ì •)
        st.markdown("#### LSTM ë”¥ëŸ¬ë‹ ëª¨ë¸")
        img = Image.open('data/LSTM_img.png')
        st.image(img, caption='LSTM ëª¨ë¸ ê²°ê³¼')
        '''
    