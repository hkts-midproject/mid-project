import streamlit as st
import pandas as pd
import numpy as np
from streamlit_option_menu import option_menu
from PIL import Image

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score


# ëª¨ë¸ë§ import
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold, cross_val_score
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from collections import Counter
import plotly.express as px
from imblearn.under_sampling import TomekLinks, RandomUnderSampler
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.pipeline import Pipeline, make_pipeline
from imblearn.combine import SMOTETomek
from xgboost import XGBClassifier


def income_run(total_df): 
    st.header('ì†Œë“ ë¶„ìœ„ ì˜ˆì¸¡')
    st.markdown("""---""")


    tab1, tab2, tab3, tab4 = st.tabs(["íŠ¹ì„± ê³µí•™", "PCA ë°ì´í„° ë¶„í¬ë„ í™•ì¸", 
                    "ìš”ì¸ ë¶„ì„", "ì†Œë“ë¶„ìœ„ ì˜ˆì¸¡ ëª¨ë¸ë§"])

    '''
    with tab1:
        st.markdown('### Feature Engineering(íŠ¹ì„± ê³µí•™)')
        st.write("ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„")
        # ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°í”„ë ˆì„ ì„¤ì • (íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì œì™¸í•œ ë…ë¦½ ë³€ìˆ˜ë“¤ë§Œ ì‚¬ìš©)
        X = total_df.drop('Income_code', axis=1)

        # ìƒìˆ˜í•­ ì¶”ê°€ (VIF ê³„ì‚°ì„ ìœ„í•´ í•„ìš”)
        X = add_constant(X)

        # VIF ê³„ì‚°
        vif_data = pd.DataFrame()
        vif_data["Feature"] = X.columns
        vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
        vf = vif_data['Feature'].drop(0)
        vv = vif_data["VIF"].drop(0)
        vif_data = pd.concat([vf, vv], axis=1)

        # VIF ê²°ê³¼ ì¶œë ¥
        st.write(vif_data)

        plt.figure(figsize=(10, 6))
        sns.barplot(x='Feature', y='VIF', data=vif_data)
        plt.title('Variance Inflation Factor (VIF)')
        plt.xticks(rotation=90)
        st.pyplot(plt)




    with tab2:
        st.markdown('### ë°ì´í„° ë¶„í¬ë„ í™•ì¸')
        st.markdown("""
            - ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ì†Œë“ë¶„ìœ„(1~10ë¶„ìœ„)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ì˜ ë¶„í¬ë„ë¥¼ í™•ì¸.
            - PCAëŠ” ë°ì´í„°ì˜ ì „ë°˜ì ì¸ ë¶„í¬ë¥¼ ì´í•´í•˜ê³ , ì£¼ì„±ë¶„ì´ ë°ì´í„°ì˜ ë³€ë™ì„±ì„ ì–´ë–»ê²Œ ì„¤ëª…í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë° ìœ ìš©í•˜ê³ 
            - t-SNEëŠ” ë°ì´í„°ì˜ í´ëŸ¬ìŠ¤í„°ë§ì´ë‚˜ êµ­ì†Œì ì¸ ë°ì´í„° êµ¬ì¡°ë¥¼ ëª…í™•í•˜ê²Œ ë“œëŸ¬ë‚´ëŠ” ë° ì í•©í•˜ê¸° ë•Œë¬¸ì— ë‘ ê°€ì§€ì˜ ë¶„í¬ë„ë¥¼ ëª¨ë‘ í™•ì¸
            
            -> ì†Œë“ ë¶„ìœ„ ê²°ì • ìš”ì¸ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ ë§ì€ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹, ëª¨ë¸ë§ ìµœì í™” ë“±ì„ í•´ ë³¸ ê²°ê³¼ ì •í™•ë„ê°€ ì¢‹ì§€ ì•Šì€ ì›ì¸ì„ ì°¾ê¸° ìœ„í•´ ì‹œë„
  
            ğŸ“Œì†Œë“ë¶„ìœ„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ PCA 2D(2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œëœ ë°ì´í„°) ë°ì´í„°ì˜ ë¶„í¬ë„ í™•ì¸ ê²°ê³¼


            - ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ì†Œë“ë¶„ìœ„(1 ~ 10ë¶„ìœ„)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ì—¬ PCA 2D ë°ì´í„°ì˜ ë¶„í¬ë„ë¥¼ í™•ì¸í–ˆì„ ë•Œ, ì•„ë˜ì˜ ê·¸ë˜í”„ì™€ ê°™ì´ ê° 1 ~ 10ë¶„ìœ„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ì˜ ë¶„í¬ê°€ ì •í™•í•˜ê²Œ êµ¬ë¶„ë˜ì§€ ì•Šì•˜ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.
            - ì˜ˆë¥¼ ë“¤ì–´, ì†Œë“ë¶„ìœ„ 1ë¶„ìœ„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ëŠ” 2ë¶„ìœ„ì˜ ë²”ìœ„ì—ë„ í¬í•¨ë˜ì–´ ìˆëŠ” ê²½ìš°ê°€ ë°œê²¬ë˜ë©° ë‹¤ë¥¸ 2, 3, 4, 5, 6, 7, 8, 9, 10 ë¶„ìœ„ ë²”ìœ„ì— í¬í•¨ëœ ë°ì´í„°ë“¤ë„ ê²½ê³„ê°€ ëª¨í˜¸í•˜ê²Œ ì„ì—¬ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.


            â­ ê²°ë¡ 

            -  ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” ë°ì´í„°ëŠ” ê·¸ ê°’ë“¤ì˜ ê²½ê³„ê°€ ëª¨í˜¸í•˜ê¸° ë•Œë¬¸ì— ì†Œë“ë¶„ìœ„ë¥¼ ì˜ˆì¸¡í•˜ê³ ì í•  ë•Œ, ì •í™•í•˜ê²Œ ê° ë¶„ìœ„ë³„ë¡œ êµ¬ë¶„ì„ í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ ì‹¤ì œ ì†Œë“ë¶„ìœ„ì™€ ìœ ì‚¬í•˜ê²Œ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤ëŠ” í•œê³„ì ì´ ë°œìƒí–ˆë‹¤.""")

        # PCA & feature scaling
        # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(total_df)


        pca = PCA(n_components=2)
        pca_data = pca.fit_transform(scaled_data)

        # ê²°ê³¼ ì‹œê°í™” (PCA 2D ì‹œê°í™”)
        plt.figure(figsize=(10, 8))
        sns.scatterplot(x=pca_data[:, 0], y=pca_data[:, 1], hue=total_df['Income_code'], palette=sns.color_palette("Spectral", 10))
        plt.title('PCA 2D data distribution')
        plt.xlabel('PCA Component 1')
        plt.ylabel('PCA Component 2')
        st.pyplot(plt)

        with st.spinner('Wait for it...'):
    

            # TSNEë¡œ ê³ ì°¨ì› ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì‹œê°í™”
            tsne = TSNE(n_components=2, random_state=42)
            tsne_data = tsne.fit_transform(scaled_data)

            # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™” (TSNE 2D ì‹œê°í™”)
            plt.figure(figsize=(10, 8))
            sns.scatterplot(x=tsne_data[:, 0], y=tsne_data[:, 1], hue=total_df['Income_code'], palette=sns.color_palette("Spectral", 10))
            plt.title('t-SNE 2D data distribution')
            plt.xlabel('TSNE Component 1')
            plt.ylabel('TSNE Component 2')
            st.pyplot(plt)


    with tab3:
        st.markdown("### ìš”ì¸ ë¶„ì„")
        st.markdown("""#### ëœë¤í¬ë ˆìŠ¤íŠ¸ íšŒê·€ ëª¨ë¸(Random Forest Regression)ì„ í†µí•œ ìš”ì¸ ë¶„ì„""")
        with st.spinner('Wait for it...'):
            # ë°ì´í„° ì¤€ë¹„ (ì˜ˆì‹œë¡œ 'Income_code'ë¥¼ íƒ€ê²Ÿ ë³€ìˆ˜ë¡œ ì„¤ì •)
            X = total_df.drop('Income_code', axis=1)
            y = total_df['Income_code']

            # í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

            # ëœë¤ í¬ë ˆìŠ¤íŠ¸ íšŒê·€ ëª¨ë¸ ì´ˆê¸°í™”
            model = RandomForestRegressor(random_state=42)

            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì •
            param_grid = {
                'n_estimators': [100, 200, 300],
                'max_depth': [None, 10, 20, 30],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }

            # GridSearchCV ì„¤ì • (MSE, RÂ², MAE ë“±ì˜ íšŒê·€ ì„±ëŠ¥ ì§€í‘œ ì‚¬ìš© ê°€ëŠ¥)
            grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')

            # GridSearchCVë¡œ ëª¨ë¸ í•™ìŠµ
            grid_search.fit(X_train, y_train)

            # ìµœì ì˜ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
            best_model = grid_search.best_estimator_
            y_pred = best_model.predict(X_test)

            # ëª¨ë¸ í‰ê°€
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            # í‰ê°€ ìˆ˜ì¹˜ ì¶œë ¥
            evaluation_metrics = {
                'Mean Squared Error (MSE)': mse,
                'Mean Absolute Error (MAE)': mae,
                'R^2 Score': r2
            }

            evaluation_metrics_df = pd.DataFrame.from_dict(evaluation_metrics, orient='index', columns=['Score'])
            st.write("ëª¨ë¸ í‰ê°€ ìˆ˜ì¹˜:")
            st.write(evaluation_metrics_df)

            # í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”
            feature_importances = pd.DataFrame(best_model.feature_importances_, index=X.columns, columns=['importance']).sort_values('importance', ascending=False)

            plt.figure(figsize=(10, 8))
            sns.barplot(x=feature_importances['importance'], y=feature_importances.index)
            plt.title('Feature Importances in Random Forest Regression')
            st.pyplot(plt)
    '''

    with tab4: 

        st.markdown("### ì†Œë“ë¶„ìœ„ ì˜ˆì¸¡ ëª¨ë¸ë§")
        st.markdown("#### ")

        tc = 'Income_code'

        # ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜
        def evaluate_model(model, X_test, y_test, model_name):
            y_pred = model.predict(X_test)
            y_pred_prob = model.predict_proba(X_test)[:, 1]

            # í‰ê°€ ì§€í‘œ ê³„ì‚°
            accuracy = metrics.accuracy_score(y_test, y_pred)
            precision = metrics.precision_score(y_test, y_pred, average='weighted')
            recall = metrics.recall_score(y_test, y_pred, average='weighted')
            f1 = metrics.f1_score(y_test, y_pred, average='weighted')
            roc_auc = metrics.roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr')

            # ê²°ê³¼ ì¶œë ¥
            st.write(f'{model_name} Evaluation Metrics:')
            st.write(f'Accuracy: {accuracy:.4f}')
            st.write(f'Precision (Weighted): {precision:.4f}')
            st.write(f'Recall (Weighted): {recall:.4f}')
            st.write(f'F1 Score (Weighted): {f1:.4f}')
            st.write(f'ROC-AUC: {roc_auc:.4f}')

            # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
            cm = metrics.confusion_matrix(y_test, y_pred)
            fig1 = plt.figure(figsize=(10, 8))
            sns.heatmap(cm, annot=True, fmt="d", linewidths=.5, cmap='Greens', square=True, cbar=False)

            # Income_codeì˜ ê³ ìœ  ê°’ì— ë§ê²Œ xticks, yticks ì„¤ì •
            labels = sorted(y.unique())  # ê³ ìœ ê°’ì„ ì •ë ¬í•˜ì—¬ ë ˆì´ë¸”ë¡œ ì‚¬ìš©
            plt.xticks(ticks=np.arange(len(labels)) + 0.5, labels=labels, rotation=45)
            plt.yticks(ticks=np.arange(len(labels)) + 0.5, labels=labels, rotation=45)

            plt.xlabel('Predicted Label')
            plt.ylabel('True Label')
            plt.title(f'{model_name} Confusion Matrix')
            st.pyplot(plt)


            # ROC Curve ì‹œê°í™”
            fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_prob, pos_label=model.classes_[1])
            fig2 = plt.figure(figsize=(8, 6))
            plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (area = {roc_auc:.4f})')
            plt.plot([0, 1], [0, 1], color='red', linestyle='--')
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'{model_name} ROC Curve')
            plt.legend(loc="lower right")
            st.pyplot(plt)

            return (fig1, fig2)

        x = total_df.drop(tc, axis=1)
        y = total_df[tc] - 1


        # ì†Œë“ë¶„ìœ„ë¥¼ 3ê°œ ë‹¨ìœ„ë¡œ êµ¬ë¶„
        def tri_y(x):
            if x < 4:
                return 0
            elif x < 7:
                return 1
            else:
                return 2


        # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        def scale(x1, x2):
            x1scaler = StandardScaler().set_output(transform="pandas")
            x2scaler = StandardScaler().set_output(transform="pandas")
            x1_s = x1scaler.fit_transform(x1)
            x2_s = x2scaler.fit_transform(x2)
            return x1_s, x2_s

        xb1, xb2, yb1, yb2 = train_test_split(x, y, stratify=y, test_size=0.3, random_state=42)

        xb1, xb2 = scale(xb1, xb2)


        ## ëª¨ë¸ë§ ì‹œì‘
        # Logistic Regressionìœ¼ë¡œ í´ë˜ìŠ¤ ë¶„ë¥˜(ì†Œë“ ë¶„ìœ„ 10ê°œ)
        
        st.markdown("#### Logistic Regression ëª¨ë¸")
        lr_x1, lr_x2, lr_y1, lr_y2 = train_test_split(x, y, stratify=y, test_size=0.3, random_state=42)

        train_scaler = StandardScaler().set_output(transform="pandas")
        test_scaler = StandardScaler().set_output(transform="pandas")

        lr_x1, lr_x2 = scale(lr_x1, lr_x2)

        kf = StratifiedKFold(n_splits=2, shuffle=False)


        lr = LogisticRegression()

        cross_val_score(lr, lr_x1, lr_y1, cv=kf, scoring='roc_auc_ovr')

        lr.fit(lr_x1, lr_y1)


        evaluate_model(lr, lr_x2, lr_y2, 'Logistic Regression')



        # XGBoost ëª¨ë¸
        st.write("XGBoost ëª¨ë¸")

        xg_x1 , xg_x2 , xg_y1 , xg_y2 = train_test_split(
            x,
            y,
            random_state=42
        )

        xg_x1, xg_x3, xg_y1, xg_y3 = train_test_split(
            xg_x1,
            xg_y1,
            random_state=42
        )




        xgb_clf = XGBClassifier(
            n_estimators=1000, # í•™ìŠµ íšŸìˆ˜
            learning_rate = 0.05, # í•™ìŠµë¥ (eta)
            max_depth=3,
            eval_metric='mlogloss'
        )
        xgb_clf.fit(xg_x1, xg_y1, verbose=True)

        evals = [
            (xg_x1, xg_y1),
            (xg_x3, xg_y3)
        ]

        xgb_clf.fit(
            xg_x1,
            xg_y1,
            early_stopping_rounds = 50,
            eval_set=evals,
            verbose=False
        )

        xgboost_fig, _ = evaluate_model(xgb_clf, xg_x2, xg_y2, 'XGBoost')


        # Random Forest ëª¨ë¸
        st.write("RandomForest ëª¨ë¸")
        rfc_x1, rfc_x2, rfc_y1, rfc_y2 = train_test_split(x, y, test_size=0.3, random_state=42)

        rfc_x1, rfc_x2 = scale(rfc_x1, rfc_x2)

        forest = RandomForestClassifier(n_estimators=200, random_state=42)
        forest.fit(rfc_x1, rfc_y1)

        y_pred = forest.predict(rfc_x2)

        fig, _  = evaluate_model(forest, rfc_x2, rfc_y2, 'Random Forest')


        st.markdown('''
                ```def gridsearch(estimator, X, Y, params =  {
                    'n_estimators' : [100, 200, 300],
                    'criterion': ['gini', 'entropy', 'logloss'],
                    'min_samples_split': [2, 3, 5, 10, 30],
                    'class_weight': ['balanced', 'balanced_subsample'],
                    'min_weight_fraction_leaf': [0.2, 0,4, 0.5]
                }):

                    grid = GridSearchCV(
                        RandomForestClassifier(random_state=42),
                        param_grid = params,
                        scoring='accuracy',
                        cv=5
                    )

                    grid.fit(X, Y)
                    print(grid.best_estimator_, grid.best_score_, grid.best_params_)```
                    
                    RandomForestClassifier(class_weight='balanced', criterion='entropy',
                       min_samples_split=10, min_weight_fraction_leaf=0,
                       n_estimators=200, random_state=42)
                       0.4228943694741741
                      {
                        'class_weight': 'balanced',
                        'criterion': 'entropy',
                        'min_samples_split': 10,
                        'min_weight_fraction_leaf': 0,
                        'n_estimators': 200
                      } ''')

        # Random Forest ëª¨ë¸ + undersampling / oversampling
        st.write("Random Forest ëª¨ë¸ + undersampling / oversampling")
        kf = StratifiedKFold(n_splits=5, shuffle=False)
        test_rfc = RandomForestClassifier(n_estimators=200, random_state=42)

        ros = RandomOverSampler(random_state=42)
        rus = RandomUnderSampler(random_state=42)
        smote = SMOTE(random_state=42)
        tomekU = TomekLinks()
        smotomek = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))

        smote_x, smote_y = smotomek.fit_resample(xb1, yb1)
        test_rfc.fit(smote_x, smote_y)

        fig_smot, _ = evaluate_model(test_rfc, xb2, yb2, 'undersampler')

        # Balanced_RandomForest ëª¨ë¸ 
        st.write("Balanced_RandomForest")
        balanced_rfc = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')
        balanced_rfc.fit(xb1, yb1)

        graph = evaluate_model(balanced_rfc, xb2, yb2, 'Balanced RFC')

        rfc_opt_nosample = RandomForestClassifier(class_weight='balanced', criterion='entropy',
                       min_samples_split=10, min_weight_fraction_leaf=0,
                       n_estimators=200, random_state=42)


        rfc_opt_nosample.fit(xb1, yb1)
        opt_fig, _ = evaluate_model(rfc_opt_nosample, xb2, yb2, 'Optimized RFC')
        st.write(cross_val_score(rfc_opt_nosample, smote_x, smote_y, cv=kf, scoring='roc_auc_ovr'))


        # LSTM ë”¥ëŸ¬ë‹ ëª¨ë¸
        st.write("Accuracy: 0.4267")
        st.write("Precision: 0.4217")
        st.write("Recall: 0.4267")
        st.write("F1 Score")
        st.write("ROC AUC Score: 0.8620")


        img = Image.open('data\LSTM_img.png')
        # ê²½ë¡œì— ìˆëŠ” ì´ë¯¸ì§€ íŒŒì¼ì„ í†µí•´ ë³€ìˆ˜ ì €ì¥
        st.image(img)
        # streamlitë¥¼ í†µí•´ ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì¤€ë‹¤.

        
            




